{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "Regression\n",
        "1) What is Simple Linear Regression\n",
        "ans 1) Simple linear regression is a statistical method that analyzes the relationship\n",
        "between one independent variable and one dependent variable using a straight line.\n",
        "\n",
        "2) What are the key assumptions of Simple Linear Regression\n",
        "ans 2) The key assumptions of simple linear regression are that the relationship between\n",
        "the independent and dependent variables is linear, errors have constant variance (homoscedasticity),\n",
        "errors are independent, and errors are normally distributed.\n",
        "\n",
        "3) What does the coefficient m represent in the equation Y=mX+c\n",
        "ans 3) In the equation y = mx + c, the coefficient 'm' represents the slope or gradient of the line.\n",
        "\n",
        "4) What does the intercept c represent in the equation Y=mX+c\n",
        "ans 4) In the equation Y = mX + c, the coefficient 'c' represents the y-intercept of the line.\n",
        "\n",
        "5) How do we calculate the slope m in Simple Linear Regression\n",
        "ans 5) In simple linear regression, the slope m is calculated using the formula: m = r * (sy / sx),\n",
        "where r is the correlation coefficient, sy is the standard deviation of the dependent variable, and\n",
        "sx is the standard deviation of the independent variable.\n",
        "\n",
        "6) What is the purpose of the least squares method in Simple Linear Regression\n",
        "ans 6) The least squares method in simple linear regression is used to find the\n",
        "\"line of best fit\" for a set of data points. It does this by minimizing the sum\n",
        "of the squared vertical distances (errors) between each data point and the line.\n",
        "In essence, it aims to find the straight line that best represents the relationship\n",
        "between the independent and dependent variables in a given dataset.\n",
        "\n",
        "7) How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "ans 7) In simple linear regression, the coefficient of determination (R²) indicates the\n",
        "proportion of the total variation in the dependent variable that is explained by the\n",
        "independent variable. It ranges from 0 to 1 (or 0% to 100%), with a higher R² indicating\n",
        "a better fit of the regression model to the data.\n",
        "\n",
        "8) What is Multiple Linear Regression\n",
        "ans 8) Multiple Linear Regression” in 200 words. - Data ScienceMultiple linear\n",
        "regression is a statistical method that models the relationship\n",
        "between a dependent variable and two or more independent variables,\n",
        "aiming to predict the dependent variable's value based on the independent variables.\n",
        "\n",
        "9- What is the main difference between Simple and Multiple Linear Regression?\n",
        "Ans9: Simple Linear Regression involves one independent variable to predict a dependent variable,\n",
        "whereas Multiple Linear Regression uses two or more independent variables.\n",
        "\n",
        "10- What are the key assumptions of Multiple Linear Regression?\n",
        "Ans 10:\n",
        "Linearity: Relationship between predictors and outcome is linear.\n",
        "*) Independence: Observations are independent.\n",
        "*) Homoscedasticity: Constant variance of residuals.\n",
        "*) Normality: Residuals are normally distributed.\n",
        "*) No multicollinearity: Predictors are not highly correlated.\n",
        "\n",
        "11 - What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Ans 11: Heteroscedasticity means the variance of residuals is not constant. It affects the model by making\n",
        "coefficient estimates inefficient and standard errors unreliable, leading to incorrect p-values.\n",
        "\n",
        "12- How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "Ans 12:\n",
        "*) Remove or combine correlated predictors\n",
        "*) Use Principal Component Analysis (PCA).\n",
        "*) Use Ridge or Lasso regression (regularization techniques).\n",
        "*) Center/standardize variables.\n",
        "\n",
        "13- What are some common techniques for transforming categorical variables for use in regression models?\n",
        "Ans 13 :\n",
        "\n",
        "*) One-Hot Encoding\n",
        "*) Label Encoding\n",
        "*) Ordinal Encoding (if order matters)\n",
        "*) Dummy Variable Encoding (avoiding dummy variable trap)\n",
        "\n",
        "14 - What is the role of interaction terms in Multiple Linear Regression?\n",
        "Answer:\n",
        "Interaction terms capture the combined effect of two variables on the outcome,\n",
        "indicating that the effect of one predictor depends on the level of another.\n",
        "\n",
        "15 - How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "Answer:\n",
        "In Simple Regression, the intercept is the predicted value when the independent variable is 0.\n",
        "In Multiple Regression, it is the predicted value when all independent variables are 0, which may not be meaningful.\n",
        "\n",
        "16 - What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "Answer:\n",
        "The slope represents the change in the dependent variable for a one-unit increase in the independent\n",
        "variable. It determines the direction and strength of the relationship.\n",
        "\n",
        "17 - How does the intercept in a regression model provide context for the relationship between variables?\n",
        "Answer:\n",
        "The intercept provides a baseline value of the dependent variable when all independent variables are 0.\n",
        "It helps anchor the regression line within the data space.\n",
        "\n",
        "18 - What are the limitations of using R² as a sole measure of model performance?\n",
        "Answer:\n",
        "\n",
        "It doesn’t indicate causation.\n",
        "\n",
        "It increases with more variables, even if they don’t improve the model.\n",
        "\n",
        "It doesn’t assess model predictive power on new data.\n",
        "\n",
        "It ignores model complexity and overfitting.\n",
        "\n",
        "19 - How would you interpret a large standard error for a regression coefficient?\n",
        "Answer:\n",
        "A large standard error suggests that the estimate of the coefficient is not precise and may not\n",
        "be statistically significant, potentially due to high variance or multicollinearity.\n",
        "\n",
        "20 - How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Answer:\n",
        "It is visible as a funnel or cone shape in residual plots. Addressing it is crucial for valid statistical\n",
        "inference, as heteroscedasticity undermines the reliability of standard errors and hypothesis tests.\n",
        "\n",
        "21 - What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "Answer:\n",
        "It suggests that additional predictors may not be contributing meaningfully to the model\n",
        "and may be inflating R² without improving model fit.\n",
        "\n",
        "22- Why is it important to scale variables in Multiple Linear Regression?\n",
        "Answer:\n",
        "Scaling (standardization) helps in interpreting coefficients, speeds up optimization, and is\n",
        "essential for regularization techniques to function properly.\n",
        "\n",
        "23 - What is polynomial regression?\n",
        "Answer:\n",
        "Polynomial regression is a form of regression where the relationship between the independent variable(s)\n",
        "and the dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "24 - How does polynomial regression differ from linear regression?\n",
        "Answer:\n",
        "Polynomial regression allows for curved (nonlinear) relationships by including powers of the predictor(s),\n",
        "whereas linear regression models only straight-line relationships.\n",
        "\n",
        "25 - When is polynomial regression used?\n",
        "Answer:\n",
        "When data shows a nonlinear trend that cannot be captured by a straight line, such as U-shaped or inverted-U patterns.\n",
        "\n",
        "26 - What is the general equation for polynomial regression?\n",
        "Answer:\n",
        "For one variable:\n",
        "y=β0​+β1​x+β2​x2+⋯+βn​xn+ϵ\n",
        "\n",
        "27- Can polynomial regression be applied to multiple variables?\n",
        "Answer:\n",
        "Yes, polynomial regression can be applied to multiple variables. It extends the concept of\n",
        "multiple linear regression by allowing for non-linear relationships between the independent\n",
        "variables and the dependent variable.  This is done by creating new predictor variables from\n",
        "the original ones, raised to different powers, and including them in the regression model\n",
        "\n",
        "28- What are the limitations of polynomial regression?\n",
        "Answer:\n",
        "Prone to overfitting with high degrees.\n",
        "Sensitive to outliers.\n",
        "Difficult to interpret.\n",
        "Can produce oscillations and poor extrapolation.\n",
        "\n",
        "29- What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "Answer:\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Adjusted R²\n",
        "\n",
        "AIC/BIC (information criteria)\n",
        "\n",
        "Residual plots\n",
        "\n",
        "RMSE on validation data\n",
        "\n",
        "30- Why is visualization important in polynomial regression?\n",
        "Answer:\n",
        "It helps assess the fit, understand the curvature, detect overfitting, and ensure the model aligns with the data pattern.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sus9z-VQyuGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31- How is polynomial regression implemented in Python?\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([1, 4, 9, 16, 25])  # Quadratic relationship\n",
        "\n",
        "# Create a pipeline for polynomial regression (degree=2)\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjp50_c96HH0",
        "outputId": "82de1c2f-6e6c-459d-ff42-d317cb53e216"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  4.  9. 16. 25.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HTenR313ytnx"
      }
    }
  ]
}